- Docker build command : docker build -t node-srv .
- Docker run command : docker run -p 5000:5000 node-srv
- Docker run with name : docker run --name node-app -p 5000:5000 node-srv
- Create a network: docker network create my-app-network

- Run the backend: docker run --name nserver --network my-app-network backend-image-name
- Run the frontend: docker run --name fclient --network my-app-network frontend-image-name
- Run db : docker run -d   --name postgres_db   -e POSTGRES_USER=postgres   -e POSTGRES_PASSWORD=postgres   -e POSTGRES_DB=notes_app   -p 5432:5432   --shm-size=128mb   -v pgdata:/var/lib/postgresql/data   postgres:18.0-alpine3.22
- Run image interactivity:  docker run -it ubuntu bash | -d detached working in the background + docker exec -it some-nginx bash open it
- Run nginx link with front build : docker run --name some-nginx -p 8000:80 -v ./front/dist:/usr/share/nginx/html nginx

- Build image with envs : docker build --build-arg BACKEND_URL=http://localhost:3000/  -t nginx-front . | define them as ARG BACKEND_URL | usage $BACKEND_URL
- Run docker with multiple replicas (add compose then run): docker compose up --scale api=3 --build (multiple instance with docker) | or put them in .yml file and just run it normally

- Docker swarm : used to orchestarte contains on multiple docker hosts (nodes) | manager node vs worker node
- Docker swarm init : command takes ip paramter
- Docker swarm join : docker swarm join-token manager | worker => to get join command with token as manage 
- Run service on the swarm :  docker service create web -p 80:80 --replicas 1 nginx:alpine | only for managers
- Show all services with : docker service ls | only for managers
- Show info of services and nodes: docker service ps web | only for managers
- In case number of replices < nr node: service will be run first on worker nodes than to managers
- Change number of replicas when service is running: docker service scale web=9 | add or remove
- Docker swarm change service version: On case we want to update the used image version in specific service => docker service update --image nginx:latest ---update-parallelism 2 --update-delay 5s web
- Docker swarm cons1: when node shut down services (out of service or docker service stop), replicas will be moved to the other nodes, when these nodes gets back the docker swarm does not re-dist the contaienrs again (complicated custom actions are needed)
- Docker swarm cons2: docker service create --name ubuntu --replicas 2 ubuntu:latest, 
    keeps retrying forever and doesn't work, cause images are stateless, and services should be stateful (takes the running command as state), when service converged ==> means the desired state is met by the contaienrs
    ubuntu container doesn't have any services, doesn't run therefore the running state will never met ==> service not converged, we can make it with add 'bash -c "while:true; do echo hello; sleep 2; done"'
    Docker swarm can have issues in case container is up but some of the inside service are running

- Docker stack: Automate services deploymenets with the desired state, file name same as docker-compose named docker-stack.yml, this file can be executed as compose (one single host)
- Docker secrets: we create ssh crtfiles, then we use the domain srt to create secrets as | docker secret create db_name domain.key or domain.crt 
    | or as txt file => echo staging | docker secret create staging token -
- Docker stack build: deploy stack deploy -c docker-stack.yml name_stack
- Docker portainer: visualize all services and stacks in good way 

- Kubernates start:
- Container management: is the process of organizing, adding, removing, or updating a significant number of containers.
- Container orchestrator: A container orchestrator is a system that automatically deploys and manages containerized apps. As part of management, the orchestrator handles scaling dynamic changes in the environment to increase or decrease the number of deployed instances of the app.  == Kubernates

- K8s Manager node Components: 
    - API server (kubectl): cli to interact with k8s API server services, manage clusters ... (+++) | also use dashboard addon | APIs in languages restful
    - etcd db: key:value stores saved memory, contains configuration and content of clusters
    - scheduler: create pods, containers, changes replicas ...
    - controller manager: compare current state of its clusters with desired state found in the etcd db
    - cloud controller manager (optional): in case deploymenet on cloud
    - virtual net: connect all nodes in the cluster into on machine
    - config map: config data of app, urls, not passwords... | secrets for password
- K8s Worker node Components: 
    - kubelet: agent communicates with manager, and execute comamnds of scheduler, reports state of node to the manager
    - kube-proxy: manage network, apis of pods, containers, services (ext, int), load balancing, reverse proxy..

- K8s base unit is pods (instead of docker container): abstraction layer | contain 0..n container | has only one ip address | 
- K8s deploymenets (instead of docker service): end user works with deploymenets | abstraction layer of pods | here we set replicas settings and update functions | (distributed over set of nodes)
- K8s service layer (load balancer): acts as load blancer layer that we can use to access the apps and deploymenets since they are distributed, users can interact with that but routing may not be clear | A Kubernetes Service is an abstraction layer which defines a logical set of Pods and enables external traffic exposure, load balancing and service discovery for those Pods.
- K8s ingress layer: manages external access, acting as an entry point for HTTP/HTTPS traffic and providing advanced routing rules to direct requests to different internal Services, better routing for users' 

- K8s pr volumes: persist volumes to save all deploymenets datas
- ConfigMap : config data of app, urls, not good for passwords and... | secrets for password

- K8s Commands:
- Minikube ip: to the cluster get ip
- check cluster on: kubectl config current-context
- show nodes:  kubectl get (nodes | deployments | services | all)
- Create deployement: kubectl create deployment kuber-test --image=gcr.io/google-samples/kubernetes-bootcamp:v1
- Access pods: activate proxy first then => http://localhost:8001/api/v1/namespaces/default/pods/kuber-test-79c9ff98f-ww5g7 show pod info => access add :8080/proxy/
- Exectute commands on pods: kubectl exec kuber-test-79c9ff98f-ww5g7 -- env | logs = kubectl logs kuber-test-79c9ff98f-ww5g7

- Expose deployment port use: kubectl expose deployment.apps/kuber-test --type='NodePort' --port 8080 (internal port) = this will create new service with auto port mapped  | should exposed to make service
- show info: kubectl describe services/test-kb
- Acces inner net with minikube: minikube service ts-kb
- Access inside cluster (minikube contianer) : clusterip:serice_exposed_port
- Rescalling deploymenets with : kubectl scale deployment.apps/t-kb --replicas 4 | load balancer will be auto 
- Another way to expose service as load balancer: kubectl expose deployment.apps/ts-kb --type=LoadBalancer --port=8080, and use minikube tunnel to access it through ext ip, found in kubectl get svc (TODO)
- Another way is to use: ingres to make the app accessible for all users (TODO)
- show current state of pod: kubectl describe pod pod-name 
- Expose Command:
    kubectl expose deployment ts-kb \
    --type=NodePort \ (ClusterIP, NodePort, LoadBalancer)
    --name=ts-kb-svc-new \
    --port=8080 \
    --target-port=8080
    --node-port=31000 (30000-32767)
- Execute command inside pod: kubectl exec -ti $POD_NAME -- curl http://localhost:8080
- update deployment version (image):  kubectl set image deployment.apps/ts-kb {Cotnainer_name}=jocatalin/kubernates-bootcamp:v2 (we get cont name by => kubectl describe deployment tskb)| can be edited through kb dashboard yml file

- For DB: we must use StatefulSet since replicating db can cause issues as they use data files

- Build with yml: kubectl apply -f config.yaml
- edit deployed config: kubectl edit deployment.apps/front-t
- get pods all info (ip): kubectl get pod -o wide
- save k8s dep in file: kubectl get deployment name-dep -o yaml > file.yml